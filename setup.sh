#!/usr/bin/env bash

set -euo pipefail

ENV_FILE=".env"
BACKUP_SUFFIX=".bak"

usage() {
    cat <<'USAGE'
Usage: ./setup.sh [-f|--force]

Generate or update the .env file with the required Airflow parameters.

Options:
  -f, --force    Overwrite values without prompting (useful for CI).
USAGE
}

confirm_overwrite() {
    local prompt=$1
    local reply=""

    if [[ ! -t 0 ]]; then
        return 1
    fi

    if ! read -r -p "$prompt (y/N): " reply; then
        return 1
    fi

    [[ $reply =~ ^[Yy]$ ]]
}

maybe_backup_env() {
    if [[ -f $ENV_FILE ]]; then
        cp "$ENV_FILE" "${ENV_FILE}${BACKUP_SUFFIX}"
        echo "üì¶ Backed up existing $ENV_FILE to ${ENV_FILE}${BACKUP_SUFFIX}"
    fi
}

read_existing_values() {
    if [[ -f $ENV_FILE ]]; then
        while IFS='=' read -r key value; do
            [[ -z $key || $key == \#* ]] && continue
            key=$(echo "$key" | tr -d ' ')
            [[ -z $key ]] && continue
            printf -v "existing_${key}" '%s' "${value%$'\r'}"
        done < "$ENV_FILE"
    fi
}

final_value() {
    local key=$1
    local var="final_${key}"
    printf '%s' "${!var:-}"
}

update_env_file() {
    cat > "$ENV_FILE" <<EOF
# Generated by setup.sh on $(date -u +"%Y-%m-%dT%H:%M:%SZ")

# --- Airflow Host/User ---
AIRFLOW_UID=$(final_value AIRFLOW_UID)
AIRFLOW_GID=$(final_value AIRFLOW_GID)
AIRFLOW_PROJ_DIR=$(final_value AIRFLOW_PROJ_DIR)

# --- Data Paths (inside containers) ---
AIRFLOW_DATA_DIR=$(final_value AIRFLOW_DATA_DIR)
TMP_DIR=$(final_value TMP_DIR)
RAW_DIR=$(final_value RAW_DIR)

# --- Ingest Source ---
CSV_NAME=$(final_value CSV_NAME)
CSV_URL=$(final_value CSV_URL)

# --- HTTP Download Tuning ---
HTTP_TIMEOUT_CONNECT=$(final_value HTTP_TIMEOUT_CONNECT)
HTTP_TIMEOUT_READ=$(final_value HTTP_TIMEOUT_READ)
HTTP_CHUNK_SIZE=$(final_value HTTP_CHUNK_SIZE)

# --- dbt CLI Paths ---
DBT_PROJECT_DIR=$(final_value DBT_PROJECT_DIR)
DBT_PROFILES_DIR=$(final_value DBT_PROFILES_DIR)
DBT_DOCS_PORT=$(final_value DBT_DOCS_PORT)

# --- dbt Project/Profile Settings ---
DBT_PROFILE_NAME=$(final_value DBT_PROFILE_NAME)
DBT_TARGET=$(final_value DBT_TARGET)
DBT_SCHEMA=$(final_value DBT_SCHEMA)
DBT_LOCATION=$(final_value DBT_LOCATION)
DBT_THREADS=$(final_value DBT_THREADS)
CSV_URI=$(final_value CSV_URI)

# --- AWS Settings for dbt Glue Catalog Access ---
DBT_ROLE_ARN=$(final_value DBT_ROLE_ARN)
DBT_REGION=$(final_value DBT_REGION)

# _PIP_ADDITIONAL_REQUIREMENTS="-r /opt/airflow/requirements.txt"
EOF
}

declare force_overwrite=0

while [[ $# -gt 0 ]]; do
    case "$1" in
        -f|--force)
            force_overwrite=1
            shift
            ;;
        -h|--help)
            usage
            exit 0
            ;;
        *)
            echo "Unknown option: $1" >&2
            usage >&2
            exit 1
            ;;
    esac
done

read_existing_values

if [[ -f $ENV_FILE && $force_overwrite -eq 0 ]]; then
    if ! confirm_overwrite "‚ö†Ô∏è  Found existing $ENV_FILE. Update managed values?"; then
        echo "‚ùå Cancelled."
        exit 1
    fi
fi

keys=(
    AIRFLOW_UID
    AIRFLOW_GID
    AIRFLOW_PROJ_DIR
    AIRFLOW_DATA_DIR
    TMP_DIR
    RAW_DIR
    CSV_NAME
    CSV_URL
    HTTP_TIMEOUT_CONNECT
    HTTP_TIMEOUT_READ
    HTTP_CHUNK_SIZE
    DBT_DOCS_PORT
    DBT_PROJECT_DIR
    DBT_PROFILES_DIR
    DBT_PROFILE_NAME
    DBT_TARGET
    DBT_SCHEMA
    DBT_LOCATION
    DBT_ROLE_ARN
    DBT_REGION
    DBT_THREADS
    CSV_URI
)

default_for() {
    case "$1" in
        AIRFLOW_UID) id -u ;;
        AIRFLOW_GID) id -g ;;
        AIRFLOW_PROJ_DIR) printf '.' ;;
        AIRFLOW_DATA_DIR) printf '/opt/airflow/data' ;;
        TMP_DIR) printf '%s/tmp' "${final_AIRFLOW_DATA_DIR:-${existing_AIRFLOW_DATA_DIR:-${AIRFLOW_DATA_DIR:-/opt/airflow/data}}}" ;;
        RAW_DIR) printf '%s/raw' "${final_AIRFLOW_DATA_DIR:-${existing_AIRFLOW_DATA_DIR:-${AIRFLOW_DATA_DIR:-/opt/airflow/data}}}" ;;
        CSV_NAME) printf 'cdc_data.csv' ;;
        CSV_URL) printf 'https://data.cdc.gov/api/views/hksd-2xuw/rows.csv?accessType=DOWNLOAD' ;;
        HTTP_TIMEOUT_CONNECT) printf '5' ;;
        HTTP_TIMEOUT_READ) printf '30' ;;
        HTTP_CHUNK_SIZE) printf '16384' ;;
        DBT_DOCS_PORT) printf '8082' ;;
        DBT_PROJECT_DIR) printf '/opt/airflow/dbt' ;;
        DBT_PROFILES_DIR) printf '/opt/airflow/dbt/profiles' ;;
        DBT_PROFILE_NAME) printf 'data_eng_assignment' ;;
        DBT_TARGET) printf 'dev' ;;
        DBT_SCHEMA) printf 'analytics' ;;
        DBT_LOCATION) printf '/opt/airflow/dbt/warehouse.duckdb' ;;
        DBT_ROLE_ARN) printf 'arn:aws:iam::<ACCOUNT_ID>:role/GlueInteractiveSessionRole' ;;
        DBT_REGION) printf 'ap-southeast-1' ;;
        DBT_THREADS) printf 1 ;;
        CSV_URI) printf '' ;;
        *) return 1 ;;
    esac
}

for key in "${keys[@]}"; do
    existing_var="existing_${key}"
    if [[ -n ${!existing_var:-} ]]; then
        printf -v "final_${key}" '%s' "${!existing_var}"
    elif [[ -n ${!key:-} ]]; then
        printf -v "final_${key}" '%s' "${!key}"
    else
        printf -v "final_${key}" '%s' "$(default_for "$key")"
    fi
done

maybe_backup_env
update_env_file

cat <<EOF
‚úÖ $ENV_FILE is ready!
   AIRFLOW_UID=$(final_value AIRFLOW_UID)  
   AIRFLOW_GID=$(final_value AIRFLOW_GID)
   AIRFLOW_PROJ_DIR=$(final_value AIRFLOW_PROJ_DIR)
   AIRFLOW_DATA_DIR=$(final_value AIRFLOW_DATA_DIR)
   DBT_PROJECT_DIR=$(final_value DBT_PROJECT_DIR)  
   DBT_PROFILES_DIR=$(final_value DBT_PROFILES_DIR)
   DBT_DOCS_PORT=$(final_value DBT_DOCS_PORT)  
   DBT_TARGET=$(final_value DBT_TARGET)
   DBT_SCHEMA=$(final_value DBT_SCHEMA)
   DBT_LOCATION=$(final_value DBT_LOCATION)
   DBT_ROLE_ARN=$(final_value DBT_ROLE_ARN)
   DBT_REGION=$(final_value DBT_REGION)
   DBT_THREADS=$(final_value DBT_THREADS)
   CSV_URI=$(final_value CSV_URI)
EOF
